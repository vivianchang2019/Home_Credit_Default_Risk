{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle Competition : Home Credit Default Risk \n\n### Using With Dimension Reduction technique\n\n> Predict how capable each applicant is of repaying a loan.\n\nReferences:<br>\n[Data Sources](https://www.kaggle.com/c/home-credit-default-risk/data) <br>\n[Start Here: A Gentle Introduction](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)<br>"},{"metadata":{},"cell_type":"markdown","source":"task:\n\n> 1. Check dataset\n\n> 2. Data Processing\n\n> 3. Dimension Reduction (PCA)  \n\n> 4. Fit different models and make prediction\n\n            a. Logistic Regression classifier\n            b. Random Forest classifier\n            c. XGBoost classifier\n\n> 5. Predict the data for kaggle submission"},{"metadata":{},"cell_type":"markdown","source":"#### kaggle submission result \n\n![submission result]()"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Check dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/home-credit-default-risk/application_train.csv')\ndf_test = pd.read_csv('/kaggle/input/home-credit-default-risk/application_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### inbalanced data\n\n91% of data is 0 (loans that were repaid on time), 8% of data is 1 (loans that were not repaid on time). "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train['TARGET'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Processing"},{"metadata":{},"cell_type":"markdown","source":"task:\n\n> a. Outlinear\n\n> b. Missing value\n\n> c. Transform categorical data\n\n> d. Scaling"},{"metadata":{},"cell_type":"markdown","source":"#### a. deal with Outlinear"},{"metadata":{},"cell_type":"markdown","source":"We find anomalies when **df_train['DAYS_EMPLOYED'] == 365243**. <br><br>\nLet's fill in the anomalous values with **np.nan** and create a new boolean column **'DAYS_EMPLOYED_ANOM'** indicating whether or not the value was anomalous."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an anomalous flag column\ndf_train['DAYS_EMPLOYED_ANOM'] = df_train[\"DAYS_EMPLOYED\"] == 365243\ndf_test['DAYS_EMPLOYED_ANOM'] = df_test[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\ndf_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\ndf_test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### b. Fill in missing value"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_missing_train = list(df_train.columns[df_train.isnull().any()])\ncol_missing_test = list(df_test.columns[df_test.isnull().any()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function from: [Impute categorical missing values in scikit-learn](https://stackoverflow.com/questions/25239958/impute-categorical-missing-values-in-scikit-learn)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import TransformerMixin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataFrameImputer(TransformerMixin):\n\n    def __init__(self):\n        \"\"\"Impute missing values.\n\n        Columns of dtype object are imputed with the most frequent value \n        in column.\n\n        Columns of other types are imputed with mean of column.\n\n        \"\"\"\n    def fit(self, X, y=None):\n\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n            index=X.columns)\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = DataFrameImputer()\nimp.fit(df_train)\n\ndf_train = imp.transform(df_train)\ndf_test = imp.transform(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing value after imputataion. \n\ndf_test[col_missing_test].isnull().sum(axis = 0)/df_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### b. transform categorical data"},{"metadata":{},"cell_type":"markdown","source":"* Label encoding  →  columns with 2 unique categories.  <br>\n* One-hot encoding  →  columns variable with more than 2 unique categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"#  columns with 2 unique categories\ncolumns_ob_two = ['NAME_CONTRACT_TYPE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'EMERGENCYSTATE_MODE', 'DAYS_EMPLOYED_ANOM']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Label encoding of 2 unique categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in columns_ob_two:\n    le = LabelEncoder()\n    df_train[col] = le.fit_transform(df_train[col])\n    df_test[col] = le.transform(df_test[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One-hot encoding of more than two unique categorical variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.get_dummies(df_train)\ndf_test = pd.get_dummies(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To remove the columns in the training data that are not in the testing data, we need to align the dataframes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\ndf_train, df_test = df_train.align(df_test, join = 'inner', axis = 1)\n\nprint('Training Features shape: ', df_train.shape)\nprint('Testing Features shape: ', df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Dimension Reduction (PCA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define training and testing data\nX_train = df_train.copy()\nX_test = df_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before using PCA, we scale the data by MinMaxScaler\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying PCA\n\npca = PCA() \npca.fit(X_train_scaled) \n\nX_train_scaled_pca = pca.transform(X_train_scaled) \nX_train_scaled_pca = pca.transform(X_test_scaled) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape: {}\".format(str(X_train_scaled_pca.shape))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The PCA class contains explained_variance_ratio_ which returns the variance caused by each of the principal components. \n\nexplained_variance = pca.explained_variance_ratio_\n\nplt.plot(np.cumsum(explained_variance))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(len(explained_variance)), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Fit different Models"},{"metadata":{},"cell_type":"markdown","source":"Task:\n\n> a. Logistic Regression classifier\n\n> b. Random Forest classifier\n\n> c. XGBoost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### a. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_log  = { 'logistic__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                'logistic__penalty': ['l1', 'l2']}\n\nmodel = Pipeline([\n        ('pca', PCA(n_components = 50)),\n        ('logistic', LogisticRegression(max_iter=10000))\n    ])\n\ngs_log = GridSearchCV(model, params_log, cv=4, n_jobs=-1, verbose=1)\ngs_log.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize\nprint('Mean Accuracy: %.3f' % gs_log.best_score_)\nprint('Config: %s' % gs_log.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### b. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_rfc  = { 'rfc__max_depth': [10, 25, 50, 75, 100],\n                'rfc__n_estimators': [50, 100, 200]}\n\nrfc = RandomForestClassifier(min_samples_split = 10, min_samples_leaf = 4, max_features = 'sqrt')\n\nmodel_rfc = Pipeline([\n        ('pca', PCA(n_components = 50)),\n        ('rfc', rfc)\n    ])\n\ngs_rfc = GridSearchCV(model_rfc, params_rfc, cv=4, n_jobs=-1, verbose=1)\ngs_rfc.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize\nprint('Mean Accuracy: %.3f' % gs_rfc.best_score_)\nprint('Config: %s' % gs_rfc.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### c. XGBoost classifier\n\n[API document](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_xgb  = { 'xgb__learning_rate': [0.05, 0.1, 0.5, 1.0],\n                'xgb__n_estimators': [100, 250, 500]}\n\nxgb = XGBClassifier()\n\nmodel_xgb = Pipeline([\n        ('pca', PCA(n_components = 50)),\n        ('xgb', xgb)\n    ])\n\ngs_xgb = GridSearchCV(model_xgb, params_xgb, cv=4, n_jobs=-1, verbose=1)\ngs_xgb.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize\nprint('Mean Accuracy: %.3f' % gs_xgb.best_score_)\nprint('Config: %s' % gs_xgb.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. predict the data for submission"},{"metadata":{},"cell_type":"markdown","source":"#### a. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions\n# Make sure to select the second column only\nresult_log = gs_log.predict_proba(X_test_scaled)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_log = pd.DataFrame({'SK_ID_CURR': df_test['SK_ID_CURR'], 'TARGET': result_log})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_log.to_csv('/kaggle/working/HCDR_submission_log_pca.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### b. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"result_rfc = gs_rfc.predict_proba(X_test_scaled)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_rfc = pd.DataFrame({'SK_ID_CURR': df_test['SK_ID_CURR'], 'TARGET': result_rfc})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_rfc.to_csv('/kaggle/working/HCDR_submission_rfc_pca.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### c. XGBoost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"result_xgb = gs_xgb.predict_proba(X_test_scaled)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_xgb = pd.DataFrame({'SK_ID_CURR': df_test['SK_ID_CURR'], 'TARGET': result_xgb})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_xgb.to_csv('/kaggle/working/HCDR_submission_xgb_pca.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}